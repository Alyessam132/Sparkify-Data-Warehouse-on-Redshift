# Sparkify Data Warehouse on Redshift

## Purpose of the database

- A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. 
- The analytics team is particularly interested in understanding what songs users are listening to. 
- Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Data used

- Song data: 's3://udacity-dend/song_data'
- Log data: 's3://udacity-dend/log_data'

### Song File

Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID

And below is an example of what a single song file looks like

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### log file

- The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

- The log files in the dataset are partitioned by year and month.

![Alt Text](log-data.png)

## Solution

-  Make ETL Pipeline to extract the song and log files
-  Create a Postgres database with tables designed to optimize queries on song play analysis 
-  Test the database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

## Database Schema

- The database schema is star schema to implement queries faster as below
    - The Fact Table is songplays table
    - The dimension tables are (users, songs, artists and time)

![Alt text](.\database-schema.png)

## ETL Pipeline

After creating the database the ETL pipeline consists of:

- getting the log and song data from s3 bucket and load the into staging_events and staging_songs tables
- inserting the data from the staging_songs table into the **songs and artists** tables
- Converting the staging_events column (ts) into timestamp
- Making the different time formats (hour, week, day, etc) and inserting them into **time** table
- Inserting the data from the staging_events table into the **songplays and users** tables

## Repo Files

- sql_queries.py : 
	-- copy the data from s3 bucket into the staging_events and staging_songs table
	-- contains the sql queries used to insert the data from the staging tables into the data warehouse tables
- create_tables.py : contains the steps to create the tables in the redshift cluster
- etl.py: is to run the functions that will load data from S3 into staging tables on Redshift and then insert that data into data warehouse tables on Redshift.
- test_cluster.ipynb : contains the test 
	1.redshift cluster creation
	2.running  create_tables.py and etl.py to load the data from s3 bucket into data warehouse
	3.redshift cluster deletion

## How to run

run test_cluster.ipynb for testing


## Result

Data has been successfly loaded from s3 bucket into data warehouse on redshift

![Alt text](.\query_dim.png)

![Alt text](.\query_fact.png)
